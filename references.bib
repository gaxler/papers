@inproceedings{Parthasarathy2022SelfsupervisedVP,
  title={Self-supervised video pretraining yields strong image representations},
  author={Nikhil Parthasarathy and S. M. Ali Eslami and Jo{\~a}o Carreira and Olivier J. H'enaff},
  year={2022},
  url={https://www.semanticscholar.org/paper/Self-supervised-video-pretraining-yields-strong-Parthasarathy-Eslami/c95527eed7758904823eb43300044fbd0cb1881c}
}

@misc{OriginalViT,
  doi = {10.48550/ARXIV.2010.11929},
  
  url = {https://arxiv.org/abs/2010.11929},
  
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{ViTFeatureAnal,
  title={Deep ViT Features as Dense Visual Descriptors},
  author={Shir Amir and Yossi Gandelsman and Shai Bagon and Tali Dekel},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.05814}
}

@inproceedings{IntriguingPropsViT,
  title={Intriguing Properties of Vision Transformers},
  author={Muzammal Naseer and Kanchana Ranasinghe and Salman Hameed Khan and Munawar Hayat and Fahad Shahbaz Khan and Ming-Hsuan Yang},
  booktitle={NeurIPS},
  year={2021},
  url={https://arxiv.org/abs/2105.10497}
}


@article{LambdaNets,
  author    = {Irwan Bello},
  title     = {LambdaNetworks: Modeling Long-Range Interactions Without Attention},
  journal   = {CoRR},
  volume    = {abs/2102.08602},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.08602},
  eprinttype = {arXiv},
  eprint    = {2102.08602},
  timestamp = {Fri, 19 Feb 2021 11:02:14 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-08602.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{SharpnessAwareMin,
       author = {{Foret}, Pierre and {Kleiner}, Ariel and {Mobahi}, Hossein and {Neyshabur}, Behnam},
        title = "{Sharpness-Aware Minimization for Efficiently Improving Generalization}",
         year = 2020,
          eid = {arXiv:2010.01412},
       eprint = {2010.01412},
}

@article{HowVITWork,
  title={How Do Vision Transformers Work?},
  author={Namuk Park and Songkuk Kim},
  journal={ArXiv},
  url={https://www.semanticscholar.org/paper/How-Do-Vision-Transformers-Work-Park-Kim/430bab3890e1e52c4c1f74900b0e408e47a1cb8f},
  year={2022},
  volume={abs/2202.06709}
}